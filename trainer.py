import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import os
import json
import time


class BrainNetworkTrainer:
    """è„‘ç½‘ç»œåˆ†ç±»è®­ç»ƒå™¨"""

    def __init__(self, model, device, learning_rate=1e-4, weight_decay=1e-4):
        self.model = model.to(device)
        self.device = device
        self.criterion = None
        # ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5, verbose=True
        )

        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [], 'accuracy': [], 'auc': [],
            'val_loss': [], 'val_accuracy': [], 'val_auc': [],
            'test_loss': [], 'test_accuracy': [], 'test_auc': []
        }

    def setup_training(self, class_weights=None):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        from utils.multitask_loss import MultiTaskLoss
        self.criterion = MultiTaskLoss()

        if class_weights is not None:
            self.class_weights = class_weights.to(self.device)
        else:
            self.class_weights = None

    def train_epoch(self, dataloader, epoch_idx, progress_bar=True):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå¢åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼‰"""
        self.model.train()
        total_loss = 0
        all_predictions = []
        all_labels = []
        all_probabilities = []

        if progress_bar:
            pbar = tqdm(dataloader, desc=f'Epoch {epoch_idx + 1} Training')
        else:
            pbar = dataloader

        for batch_idx, (features, labels) in enumerate(pbar):
            features = features.to(self.device)
            labels = labels.to(self.device)

            self.optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            predictions, model_outputs = self.model(features)

            # æ£€æŸ¥é¢„æµ‹æ˜¯å¦åŒ…å«NaN
            if torch.isnan(predictions).any():
                print(f"è­¦å‘Š: ç¬¬{epoch_idx}è½®ç¬¬{batch_idx}æ‰¹æ¬¡çš„é¢„æµ‹åŒ…å«NaNï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                continue

            # è®¡ç®—æŸå¤±
            loss, loss_components = self.criterion(predictions, labels, model_outputs)

            # æ£€æŸ¥æŸå¤±æ˜¯å¦åŒ…å«NaN
            if torch.isnan(loss):
                print(f"è­¦å‘Š: ç¬¬{epoch_idx}è½®ç¬¬{batch_idx}æ‰¹æ¬¡çš„æŸå¤±ä¸ºNaNï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                continue

            total_loss += loss.item()

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆæ›´ä¸¥æ ¼çš„è£å‰ªï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)

            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaN
            has_nan_grad = False
            for name, param in self.model.named_parameters():
                if param.grad is not None and torch.isnan(param.grad).any():
                    print(f"è­¦å‘Š: å‚æ•° {name} çš„æ¢¯åº¦åŒ…å«NaN")
                    has_nan_grad = True
                    break

            if not has_nan_grad:
                self.optimizer.step()
            else:
                print(f"è­¦å‘Š: æ£€æµ‹åˆ°NaNæ¢¯åº¦ï¼Œè·³è¿‡å‚æ•°æ›´æ–°")

            # æ”¶é›†é¢„æµ‹ã€æ ‡ç­¾å’Œæ¦‚ç‡
            all_predictions.append(predictions.detach().cpu())
            all_labels.append(labels.detach().cpu())

            # è®¡ç®—æ¦‚ç‡ï¼ˆç”¨äºAUCï¼‰ï¼Œå¢åŠ æ•°å€¼ç¨³å®šæ€§
            with torch.no_grad():
                probabilities = torch.softmax(predictions, dim=1)

                # æ£€æŸ¥æ¦‚ç‡æ˜¯å¦åŒ…å«NaN
                if torch.isnan(probabilities).any():
                    print(f"è­¦å‘Š: æ¦‚ç‡åŒ…å«NaNï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒæ›¿ä»£")
                    probabilities = torch.ones_like(probabilities) / probabilities.shape[1]

                all_probabilities.append(probabilities.detach().cpu())

            if progress_bar and batch_idx % 10 == 0:
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'CLS': f'{loss_components["cls_loss"]:.4f}'
                })

        # è®¡ç®—epochæŒ‡æ ‡
        from utils.metrics import ClassificationMetrics

        # å®‰å…¨åœ°è·å–ç±»åˆ«æ•°
        try:
            num_classes = self.model.num_classes
        except AttributeError:
            num_classes = 2

        metrics_calc = ClassificationMetrics(num_classes=num_classes)

        if len(all_predictions) == 0:
            print(f"è­¦å‘Š: ç¬¬{epoch_idx}è½®æ²¡æœ‰æœ‰æ•ˆæ‰¹æ¬¡ï¼Œä½¿ç”¨é»˜è®¤æŒ‡æ ‡")
            return 1.0, {'accuracy': 0.5, 'auc': 0.5}, {'cls_loss': 1.0, 'div_loss': 0.0, 'sparse_loss': 0.0,
                                                        'consistency_loss': 0.0, 'smooth_loss': 0.0}

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®
        all_predictions = torch.cat(all_predictions)
        all_labels = torch.cat(all_labels)
        all_probabilities = torch.cat(all_probabilities)

        # æ›´æ–°æŒ‡æ ‡ï¼ˆä¼ å…¥æ¦‚ç‡ï¼‰
        metrics_calc.update(all_predictions, all_labels, all_probabilities)
        metrics = metrics_calc.compute()

        epoch_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 1.0
        epoch_accuracy = metrics['accuracy']
        epoch_auc = metrics.get('auc', 0.5)

        # æ›´æ–°å†å²
        self.train_history['loss'].append(epoch_loss)
        self.train_history['accuracy'].append(epoch_accuracy)
        self.train_history['auc'].append(epoch_auc)

        return epoch_loss, metrics, loss_components

    def validate(self, dataloader, mode='val'):
        """éªŒè¯/æµ‹è¯•æ¨¡å‹ï¼ˆå¢åŠ æ•°å€¼ç¨³å®šæ€§ï¼‰"""
        self.model.eval()
        all_predictions = []
        all_labels = []
        all_probabilities = []
        total_val_loss = 0
        valid_batches = 0

        with torch.no_grad():
            for features, labels in tqdm(dataloader, desc=f'{mode.capitalize()}idation'):
                features = features.to(self.device)
                labels = labels.to(self.device)

                predictions, model_outputs = self.model(features)

                # è·³è¿‡åŒ…å«NaNçš„é¢„æµ‹
                if torch.isnan(predictions).any():
                    print(f"è­¦å‘Š: {mode}é›†ä¸­æ£€æµ‹åˆ°NaNé¢„æµ‹ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                    continue

                # è®¡ç®—éªŒè¯æŸå¤±
                val_loss, _ = self.criterion(predictions, labels, model_outputs)

                if not torch.isnan(val_loss):
                    total_val_loss += val_loss.item()
                    valid_batches += 1

                # æ”¶é›†æ•°æ®
                all_predictions.append(predictions.cpu())
                all_labels.append(labels.cpu())

                # è®¡ç®—æ¦‚ç‡ï¼Œå¢åŠ ç¨³å®šæ€§
                probabilities = torch.softmax(predictions, dim=1)
                if torch.isnan(probabilities).any():
                    probabilities = torch.ones_like(probabilities) / probabilities.shape[1]

                all_probabilities.append(probabilities.cpu())

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ‰¹æ¬¡ï¼Œè¿”å›é»˜è®¤å€¼
        if valid_batches == 0:
            print(f"è­¦å‘Š: {mode}é›†æ²¡æœ‰æœ‰æ•ˆæ‰¹æ¬¡")
            return 1.0, {'accuracy': 0.5, 'auc': 0.5}

        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        from utils.metrics import ClassificationMetrics

        try:
            num_classes = self.model.num_classes
        except AttributeError:
            num_classes = 2

        metrics_calc = ClassificationMetrics(num_classes=num_classes)

        # åˆå¹¶æ•°æ®
        all_predictions = torch.cat(all_predictions)
        all_labels = torch.cat(all_labels)
        all_probabilities = torch.cat(all_probabilities)

        # æ›´æ–°æŒ‡æ ‡ï¼ˆä¼ å…¥æ¦‚ç‡ï¼‰
        metrics_calc.update(all_predictions, all_labels, all_probabilities)
        metrics = metrics_calc.compute()

        avg_val_loss = total_val_loss / valid_batches

        # æ›´æ–°å†å²
        if mode == 'val':
            self.train_history['val_loss'].append(avg_val_loss)
            self.train_history['val_accuracy'].append(metrics['accuracy'])
            self.train_history['val_auc'].append(metrics.get('auc', 0.5))
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆä»…åœ¨éªŒè¯é›†ä¸Šï¼‰
            self.scheduler.step(avg_val_loss)
        elif mode == 'test':
            self.train_history['test_loss'].append(avg_val_loss)
            self.train_history['test_accuracy'].append(metrics['accuracy'])
            self.train_history['test_auc'].append(metrics.get('auc', 0.5))

        return avg_val_loss, metrics

    # å…¶ä½™æ–¹æ³•ä¿æŒä¸å˜...
    def save_checkpoint(self, epoch, path):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_history': self.train_history
        }
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save(checkpoint, path)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}")

    def load_checkpoint(self, path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.train_history = checkpoint['train_history']
        print(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {path}, ä»epoch {checkpoint['epoch']}ç»§ç»­è®­ç»ƒ")
        return checkpoint['epoch']


def train_complete_model(config):
    """å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°ï¼‰"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from utils.dataloader import create_data_loaders
    train_loader, val_loader, test_loader, class_weights = create_data_loaders(
        config=config,
        shuffle=True,
        num_workers=config['training'].get('num_workers', 0)
    )

    # åˆ›å»ºæ¨¡å‹
    from model.classifier import BrainNetworkClassifier
    model = BrainNetworkClassifier(
        num_rois=config['model']['num_rois'],
        num_scales=config['model']['num_scales'],
        num_bands=config['model']['num_bands'],
        feat_dim=config['model']['feat_dim'],
        hidden_dim=config['model']['hidden_dim'],
        num_classes=config['model']['num_classes']
    )
    model.enable_attention_saving(True)

    # ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    learning_rate = config['training'].get('learning_rate', 1e-4)
    # å¦‚æœä¹‹å‰æœ‰é—®é¢˜ï¼Œè‡ªåŠ¨é™ä½å­¦ä¹ ç‡
    learning_rate = min(learning_rate, 1e-4)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BrainNetworkTrainer(
        model, device,
        learning_rate=learning_rate,
        weight_decay=config['training'].get('weight_decay', 1e-4)
    )
    trainer.setup_training(class_weights)

    # ç¡®ä¿resultç›¸å…³ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(config['paths']['best_model']), exist_ok=True)
    os.makedirs(config['paths']['checkpoint_dir'], exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    best_val_auc = 0
    num_epochs = config['training']['num_epochs']
    save_interval = config['training']['save_interval']

    print("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(num_epochs):
        epoch_start_time = time.time()

        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")

        # è®­ç»ƒ
        train_loss, train_metrics, loss_components = trainer.train_epoch(
            train_loader, epoch
        )

        # éªŒè¯
        val_loss, val_metrics = trainer.validate(val_loader, mode='val')

        # æµ‹è¯•
        test_loss, test_metrics = trainer.validate(test_loader, mode='test')

        epoch_time = time.time() - epoch_start_time

        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"Epoch[{epoch + 1}/{num_epochs}] | "
              f"Train Loss: {train_loss:.3f} | Train Accuracy: {train_metrics['accuracy'] * 100:.3f}% | "
              f"Val Loss: {val_loss:.3f} | Val Accuracy: {val_metrics['accuracy'] * 100:.3f}% | Val AUC: {val_metrics.get('auc', 0):.4f} | "
              f"Test Loss: {test_loss:.3f} | Test Accuracy: {test_metrics['accuracy'] * 100:.3f}% | Test AUC: {test_metrics.get('auc', 0):.4f} | "
              f"Time: {epoch_time:.1f}s")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_auc = val_metrics.get('auc', 0)
        if current_auc > best_val_auc:
            best_val_auc = current_auc
            trainer.save_checkpoint(epoch, config['paths']['best_model'])
            print(f"âœ¨ æ–°çš„æœ€ä½³æ¨¡å‹! Val AUC: {best_val_auc:.4f}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = os.path.join(
                config['paths']['checkpoint_dir'],
                f"checkpoint_epoch_{epoch + 1}.pth"
            )
            trainer.save_checkpoint(epoch, checkpoint_path)

    # ä¿å­˜è®­ç»ƒå†å²
    with open(config['paths']['train_history'], 'w') as f:
        serializable_history = {
            k: [v.item() if isinstance(v, np.ndarray) else v for v in vals]
            for k, vals in trainer.train_history.items()
        }
        json.dump(serializable_history, f, indent=2)
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {config['paths']['train_history']}")

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")

    return trainer